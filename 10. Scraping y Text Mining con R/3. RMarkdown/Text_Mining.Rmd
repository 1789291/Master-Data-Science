---
title: "Text Mining"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Este notebook contiene el trabajo realizado en clase. Para la explicación teórica ver apuntes pdf y html.

```{r, message=FALSE}
library(stringr)
```

Haremos un análisis de frecuencia de las palabras.

## Manipulación y análisis básicos de texto

Veremos esto con R Base y con `stringr`.

* Función `gsub`

```{r}
# Sustituye el primer argumento por el segundo. Puedo meterle expresiones regulares
gsub("h", "H", c("hola", "búho"))

```

```{r}
# Sustituimos la primera h

gsub("^h", "H", c("hola", "búho"))

```

```{r}
# stringr

str_replace("Búho", "h", "H")
```

```{r}
# Sustituir el final de una palabra por algo (por ejemplo, poner palabras en plural)

gsub("$","s", c("pera", "sandía", "plátano") )  # El $ es el final de una palabra
```

* Función `grep`: me permite detectar patrones en textos

```{r}
grep("a$", c("pera", "sandía", "plátano"))

# Me dice en qué posiciones tengo cosas que terminan con a
```

```{r}
# Si quiero extraer las que son:

grep("a$", c("pera", "sandía", "plátano"), value = TRUE)

```

**Ejemplo**: todos los colores que terminan con blue:

```{r}
colores <- head(colors(),10)
colores
```

```{r}
grep("blue$", colores, value = TRUE)
```

*Función `paste`:

```{r}
paste("A", 1:6, sep = ",")

```

```{r}
paste("A", 1:6, collapse = ",")  # collapse es la separación

```

A continuación una aplicación de uso del paquete `stringr` donde se describe cómo se distribuyen las Medallas Fields (el “nobel” en matemáticas) entre los países, utilizando la información proporcionada por la wikipedia.

Empezamos extrayendo la tabla de interés desde la Wikipedia:

```{r}
require(rvest)
mfield<-read_html("https://es.wikipedia.org/w/index.php?title=Medalla_Fields&oldid=103644843")
mfield %>% html_nodes("table") 
tabla <- mfield %>% html_nodes("table") %>% .[[2]] %>% html_table(header=TRUE)
knitr:::kable(tabla %>% head(5))

```

Los países están entre paréntesis. También hay un problema y es que algunos científicos tienen doble nacionalidad.

```{r}
require(tidyverse)
tmp <- tabla$Medallistas %>% str_extract("\\([^()]+\\)") #extrae contenido entre parentesis 
tmp <- substring(tmp,2,nchar(tmp)-1) 
paises<- tmp %>% str_split_fixed(" y ", 2) %>% str_trim() %>% c()  # Doble nacionalidad

```

Representación de distribución de medallas entre los países:

```{r}
freq=c(table(paises))[-1] #el -1 es para quitar la frecuencia de ""
qplot(freq,reorder(names(freq),freq),ylab="paises")

```


## Creación de un Corpus con tidytext

El formato de texto `tidy` es básicamente una tabla con un **token** por fila. Este formato se presta muy bien a la minería de datos textuales.

### Tokenización con la función `unnest_tokens`

```{r}
texto<-c("Eso es insultar al lector, es llamarle torpe","Es decirle: ¡fíjate, hombre, fíjate, que aquí hay intención!","Y por eso le recomendaba yo a un señor que escribiese sus artículos todo en bastardilla","Para que el público se diese cuenta de que eran intencionadísimos desde la primera palabra a la última.")
texto

```

Para analizar este tipo de información textual con tidytext, se le da un formato de tabla:
```{r}
require(tidyverse)
texto_df <- data_frame(fila = 1:4, texto = texto)
texto_df


```

Todavía esta tabla no permite un análisis del texto. No podemos filtrar las palabras o calcular sus frecuencias, puesto que cada fila se compone de varias palabras combinadas. Necesitamos transformarla de manera que un token por fila .

A menudo, el token es una secuencia de caracteres entre dos separadores. Un separador puede ser un “blanco”, una puntuación, un paréntesis, etc. Para segmentar el texto en tokens individuales y transformarlo en una estructura de datos utilizamos aquí la función `unnest_tokens` del paquete `tidytext`.

```{r}
require(tidytext)
texto_df %>% unnest_tokens(palabra, texto)

# Me separa todo y me lo pone por defecto en minúscula. También me conserva la fila en la que estaba.

# Los argumentos de la función se pueden cambiar (tema mayúsculas, etc...)
```

Por ejemplo: descomponer en grupos de dos palabras:

```{r}
texto_df %>% unnest_tokens(palabra, texto, token = "ngrams", n = 2)
```

### Tokenización de la obra de Jane Austen

```{r}
require(janeaustenr)
libros <- austen_books() %>%
  group_by(book) %>%
  mutate(linenumber = row_number(),
         chapter = cumsum(str_detect(text, regex("^chapter [[:digit:]ivxlc]", ignore_case=TRUE)))) %>%
  ungroup()

# Agrupo por libro
# Creo un campo que es el número de línea y
# Otro en el que cada vez que me encuentro con algo del estilo de CHAPTER IV sumo 1 a la variable


head(libros)

```

```{r}
tokens <- libros %>% unnest_tokens(word, text)
tokens

```

Ahora ya tenemos lista la tabla. Vamos a analizar las frecuencias de las palabras, la cantidad de veces que se repiten.

## Análisis de frecuencias de tokens

Una opción es coger `stop_words`.

```{r}
tokens <- tokens %>% anti_join(stop_words)

```

```{r}
head(stop_words)

```
Para palabras en español:

```{r}
require(stopwords)
# stopwords(language = "es")
```


La otra manera es aprovechando el concepto de corpus: las palabras que mejor caractericen a una novela serán las que sean muy frecuentes en esa novela y no tanto en el resto de novelas del corpus. Siempre que tengamos un corpus (entendiendo como corpus un conjunto de textos) conviene hacerlo aprovechándolo.

En primer lugar eliminamos las stop words.


```{r}
freq <- tokens %>% count(word, sort = TRUE) 

```

```{r}
require(ggplot2)
freq %>%
  filter(n > 600) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip()

```

Nube de palabras:

```{r}
require(wordcloud)
wordcloud(words = freq$word, freq = freq$n, min.freq = 300,
          max.words=100, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))

```

Esto representa las palabras que se repiten mucho en todo el corpus (es decir, en todas las novelas). Si quiero ver qué palabras se repiten mucho en CADA novela:


**Frecuencia inversa de documentos**: lo que habíamos comentado de utilizar la frecuencia del corpus:. Voy a asociar a cada palabra una medida de lo común que es dentro del corpus.


```{r}
book_words <- austen_books() %>% unnest_tokens(word, text) %>%
  count(book, word, sort = TRUE) %>%
  ungroup()

freq_rel <- book_words %>% bind_tf_idf(word, book, n)
freq_rel

```


```{r}
freq_rel %>% arrange(desc(tf_idf))

```

Para cada palabra, un idf de 0 me dice que está en todas las novelas. El tf me dice la frecuencia de cada palabra en el corpus.

La medida tf-idf mide hasta que punto una palabra caracteriza un documento dado dentro de una colección (o corpus) al cual pertenece dicho documento.


```{r}
freq_rel %>% arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>% 
  group_by(book) %>% 
  top_n(15) %>% 
  ungroup() %>%
  ggplot(aes(word, tf_idf, fill = book)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~book, ncol = 2, scales = "free") +
  coord_flip()

```

Lo que quería es caracterizar las novelas (es decir, diferenciarlas) en base a sus palabras. Para ello, quiero ver en cada novela las que más se repiten, siempre que se repitan poco en las otras. Por eso utilizamos el tf.idf.

Vemos que son todo nombres. Tiene sentido, pero no nos dejan ver de qué habla el libro. 

Una opción sería quitarlos a mano. Otra sería, a la hora de tokenizar, no poner todo en minúsculas y quitar las palabras con mayúscula (así nos quitamos los nombres). Nos quitaremos otras muchas palabras, pero en un corpus grande no será demasiado error.

